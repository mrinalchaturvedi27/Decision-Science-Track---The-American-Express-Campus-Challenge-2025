![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![LightGBM](https://img.shields.io/badge/LightGBM-2.3+-lightgreen?logo=leaflet)
![Optuna](https://img.shields.io/badge/Optuna-Bayesian--Opt-lightgrey?logo=python)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)
![IIT Kanpur](https://img.shields.io/badge/IIT-Kanpur-red?logo=google-scholar)

# ğŸ“Š American Express Campus Challenge 2025 â€” Decision Science Track  

**Team:** *IIT Kanpur (Top 50 Finalist out of 11,753 teams)*  
**Problem Statement:** Improve **customer engagement** by optimizing the **ranking of offers** shown to users.  

---

## ğŸš€ Project Overview  
Traditional click-prediction models treat each offer independently, missing the **business-critical objective**: *ranking offers by relevance*.  
Our solution delivers a **ranking system** that boosts offer engagement by **+195% (MAP@7)** using a pipeline of **feature engineering, smart sampling, and model optimization**.  

---

## ğŸ”§ Methodology  

### 1ï¸âƒ£ Data Sampling & Balancing  
- Designed a **smart negative sampling strategy** (5:1 ratio) to preserve **authentic user behavior**.  
- Retained all clicked offers, sampled subset of non-clicked offers â†’ avoided synthetic bias (SMOTE/oversampling).  

### 2ï¸âƒ£ Feature Engineering (51 Features)  
- **Temporal Signals**: offer age, recency, daily/hourly activity patterns.  
- **Behavioral Signals**: campaign fatigue, offer fatigue, user CTR, impression gaps.  
- **Semantic Signals**: TF-IDF embeddings on product/service, campaign codes, and offer types.  
- **Spend Patterns**: user-level spend mean, std, and variability.  

### 3ï¸âƒ£ Modeling  
- **Algorithm**: LightGBM Ranker (LambdaRank objective).  
- **Validation**: 5-fold GroupKFold CV by session/user â†’ no temporal leakage.  
- **Hyperparameter Tuning**: Optuna (847 trials, TPE sampler).  

### 4ï¸âƒ£ Why This Works  
âœ” Preserves real user preferences  
âœ” Optimizes directly for ranking relevance (MAP@7)  
âœ” Captures behavioral + semantic + temporal signals  

---

## ğŸ“ˆ Results  

| Iteration                                | MAP@7 | Î” vs Baseline |
|------------------------------------------|-------|---------------|
| Baseline LightGBM                        | 0.180 | â€”             |
| + Feature Engineering                    | 0.444 | +146.7%       |
| + Smart Sampling + Optuna Tuning         | 0.531 | +195.0%       |

- **Final Score:** MAP@7 = **0.531**  
- **Top 50 / 11,753 teams nationwide**  
- **Business Impact**:  
  - â†‘ Engagement: +40â€“60% click-through uplift  
  - â†‘ Customer Lifetime Value: +15â€“25% estimated increase  
  - â†“ Training Cost: â€“80% vs naive approaches  

---

## ğŸ— Tech Stack  
- **Python** (pandas, numpy, scikit-learn, LightGBM, Optuna)  
- **Visualization**: Matplotlib, SHAP  
- **ML Paradigm**: Learning-to-Rank (LambdaRank objective)  

---

## ğŸ“‚ Repository Structure  

