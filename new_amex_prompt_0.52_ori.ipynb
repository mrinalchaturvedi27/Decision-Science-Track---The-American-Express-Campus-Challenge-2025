{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T07:51:13.676023Z",
     "iopub.status.busy": "2025-07-16T07:51:13.675455Z",
     "iopub.status.idle": "2025-07-16T07:51:13.682177Z",
     "shell.execute_reply": "2025-07-16T07:51:13.681190Z",
     "shell.execute_reply.started": "2025-07-16T07:51:13.675998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 0. Setup and Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import gc\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Configuration ---\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('robust_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/teamspace/studios/this_studio/\"\n",
    "\n",
    "def safe_save(obj, name):\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    with open(os.path.join(CACHE_DIR, f\"{name}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def safe_load(name):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Initial Preparation ---\n",
    "def load_and_prepare_data(base_path='/teamspace/uploads/'):\n",
    "    \"\"\"Loads all data, standardizes IDs, and fixes basic data types.\"\"\"\n",
    "    logger.info(\"Starting data loading and preparation.\")\n",
    "    try:\n",
    "        train = pd.read_parquet(base_path + 'train_data.parquet')\n",
    "        logger.info(f\"Loaded train_data.parquet with shape: {train.shape}\")\n",
    "        test = pd.read_parquet(base_path + 'test_data.parquet')\n",
    "        logger.info(f\"Loaded test_data.parquet with shape: {test.shape}\")\n",
    "        events = pd.read_parquet(base_path + 'add_event.parquet')\n",
    "        logger.info(f\"Loaded add_event.parquet with shape: {events.shape}\")\n",
    "        trans = pd.read_parquet(base_path + 'add_trans.parquet')\n",
    "        logger.info(f\"Loaded add_trans.parquet with shape: {trans.shape}\")\n",
    "        offers = pd.read_parquet(base_path + 'offer_metadata.parquet')\n",
    "        logger.info(f\"Loaded offer_metadata.parquet with shape: {offers.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Standardize IDs to string for safe merging\n",
    "    for df_name, df in zip(['train', 'test', 'events', 'trans', 'offers'], [train, test, events, trans, offers]):\n",
    "        for col in ['id2', 'id3']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "    logger.info(\"Standardized 'id2' and 'id3' columns to string type.\")\n",
    "\n",
    "    # Prepare target variable\n",
    "    train['y'] = pd.to_numeric(train['y'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    train['id4'] = pd.to_datetime(train['id4'], errors='coerce')\n",
    "    test['id4'] = pd.to_datetime(test['id4'], errors='coerce')\n",
    "    trans['f370'] = pd.to_datetime(trans['f370'], errors='coerce')\n",
    "    offers['id12'] = pd.to_datetime(offers['id12'], errors='coerce') # start date\n",
    "    offers['id13'] = pd.to_datetime(offers['id13'], errors='coerce') # end date\n",
    "    logger.info(\"Converted timestamp columns to datetime objects.\")\n",
    "    \n",
    "    logger.info(f\"Data preparation complete. Final shapes - Train: {train.shape}, Test: {test.shape}\")\n",
    "    return train, test, events, trans, offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T07:51:13.683532Z",
     "iopub.status.busy": "2025-07-16T07:51:13.683189Z",
     "iopub.status.idle": "2025-07-16T07:51:13.706265Z",
     "shell.execute_reply": "2025-07-16T07:51:13.705497Z",
     "shell.execute_reply.started": "2025-07-16T07:51:13.683497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Feature Engineering ---\n",
    "def create_features(df, offers, trans, events):\n",
    "    \"\"\"Creates a rich set of features for the ranking model.\"\"\"\n",
    "    logger.info(f\"Starting feature engineering on dataframe with shape {df.shape}.\")\n",
    "    \n",
    "    # Merge offer metadata first\n",
    "    df = df.merge(offers, on='id3', how='left')\n",
    "    logger.info(f\"Shape after merging offers: {df.shape}\")\n",
    "\n",
    "    # --- A. Basic Temporal Features ---\n",
    "    df['dayofweek'] = df['id4'].dt.dayofweek\n",
    "    df['hour'] = df['id4'].dt.hour\n",
    "    logger.info(\"Created basic temporal features: dayofweek, hour.\")\n",
    "    \n",
    "    # --- B. Industry Code (id8) Alignment Features ---\n",
    "    logger.info(\"Creating id8 (Industry) alignment features.\")\n",
    "    customer_industry_spend = trans.groupby(['id2', 'id8'])['f367'].sum().reset_index()\n",
    "    customer_total_spend = trans.groupby('id2')['f367'].sum().reset_index().rename(columns={'f367': 'total_spend'})\n",
    "    customer_industry_spend = customer_industry_spend.merge(customer_total_spend, on='id2', how='left')\n",
    "    customer_industry_spend['industry_spend_proportion'] = customer_industry_spend['f367'] / customer_industry_spend['total_spend']\n",
    "    \n",
    "    df = df.merge(customer_industry_spend[['id2', 'id8', 'industry_spend_proportion']], on=['id2', 'id8'], how='left')\n",
    "    df['industry_match'] = df['industry_spend_proportion'].notna().astype(int)\n",
    "    \n",
    "    # --- C. Behavioral, Fatigue & Session Features ---\n",
    "    logger.info(\"Creating behavioral and fatigue features.\")\n",
    "    df = df.sort_values(by=['id2', 'id4']).reset_index(drop=True)\n",
    "    df['offer_fatigue_user'] = df.groupby(['id2', 'id3']).cumcount()\n",
    "    df['campaign_fatigue_user'] = df.groupby(['id2', 'id9']).cumcount()\n",
    "    df['session_impression_gap_secs'] = df.groupby('id2')['id4'].diff().dt.total_seconds()\n",
    "    \n",
    "    # --- D. Cold-Start & Global Features ---\n",
    "    logger.info(\"Creating cold-start and global features.\")\n",
    "    df['offer_age_days'] = (df['id4'] - df['id12']).dt.days\n",
    "    df['offer_duration_days'] = (df['id13'] - df['id12']).dt.days\n",
    "    \n",
    "    # --- E. Transaction and Event Aggregations ---\n",
    "    logger.info(\"Creating transaction and event aggregations.\")\n",
    "    agg_trans = trans.groupby('id2')['f367'].agg(['mean', 'std', 'sum', 'count']).reset_index()\n",
    "    agg_trans.columns = ['id2', 'trans_mean', 'trans_std', 'trans_sum', 'trans_count']\n",
    "    df = df.merge(agg_trans, on='id2', how='left')\n",
    "\n",
    "    events['click_flag'] = events['id7'].notnull().astype(int)\n",
    "    event_aggs = events.groupby('id2').agg(click_rate=('click_flag', 'mean'), click_count=('click_flag', 'sum'))\n",
    "    df = df.merge(event_aggs, on='id2', how='left')\n",
    "\n",
    "    # --- F. Interaction Features ---\n",
    "    df['offer_popularity'] = df.groupby('id3')['id2'].transform('count')\n",
    "    df['customer_offer_frequency'] = df.groupby('id2')['id3'].transform('count')\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    logger.info(f\"Feature engineering complete. Final shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Text & Target Encoding ---\n",
    "def apply_text_and_target_encoding(train_df, test_df, target_col='y', group_col='id2'):\n",
    "    \"\"\"Applies TF-IDF and GroupKFold Target Encoding.\"\"\"\n",
    "    logger.info(\"Applying TF-IDF and Target Encoding.\")\n",
    "    \n",
    "    # TF-IDF on 'f378'\n",
    "    tfidf = TfidfVectorizer(max_features=30, min_df=5, dtype=np.float32)\n",
    "    tfidf_train = tfidf.fit_transform(train_df['f378'].fillna('missing'))\n",
    "    tfidf_test = tfidf.transform(test_df['f378'].fillna('missing'))\n",
    "    \n",
    "    tfidf_train_df = pd.DataFrame(tfidf_train.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_train.shape[1])])\n",
    "    tfidf_test_df = pd.DataFrame(tfidf_test.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_test.shape[1])])\n",
    "    \n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), tfidf_train_df], axis=1)\n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), tfidf_test_df], axis=1)\n",
    "    logger.info(f\"Applied TF-IDF. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # GroupKFold Target Encoding for 'id3'\n",
    "    logger.info(\"Starting GroupKFold Target Encoding for 'id3'.\")\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    train_df['id3_te'] = 0.0\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_df, train_df[target_col], train_df[group_col])):\n",
    "        logger.info(f\"Processing fold {fold+1}/5 for target encoding.\")\n",
    "        means = train_df.iloc[tr_idx].groupby('id3')[target_col].mean()\n",
    "        train_df.loc[val_idx, 'id3_te'] = train_df.loc[val_idx, 'id3'].map(means).fillna(global_mean)\n",
    "        \n",
    "    full_train_means = train_df.groupby('id3')[target_col].mean()\n",
    "    test_df['id3_te'] = test_df['id3'].map(full_train_means).fillna(global_mean)\n",
    "    logger.info(\"Target encoding complete.\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T07:51:13.708835Z",
     "iopub.status.busy": "2025-07-16T07:51:13.708543Z",
     "iopub.status.idle": "2025-07-16T07:51:13.728110Z",
     "shell.execute_reply": "2025-07-16T07:51:13.727195Z",
     "shell.execute_reply.started": "2025-07-16T07:51:13.708809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. Model Training with Optuna ---\n",
    "def train_lgbm_with_optuna(train_df, test_df, features, n_trials=9):\n",
    "    \"\"\"Trains a LightGBM Ranker with Optuna for hyperparameter tuning.\"\"\"\n",
    "    logger.info(f\"Starting model training with Optuna for {n_trials} trials.\")\n",
    "    \n",
    "    X = train_df[features].copy()\n",
    "    y = train_df['y'].copy()\n",
    "    X_test = test_df[features].copy()\n",
    "    groups = train_df['id2'].copy()\n",
    "    \n",
    "    logger.info(f\"Training data shape (X): {X.shape}\")\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            median_val = X[col].median()\n",
    "            X[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "    logger.info(\"Filled NaN values using column medians.\")\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'lambdarank', 'metric': 'map', 'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 7, 11),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 50, 150),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-6, 5.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-6, 5.0, log=True),\n",
    "            'random_state': 42, 'verbose': -1, 'bagging_freq': 1\n",
    "        }\n",
    "        \n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "        oof_map_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            group_tr = groups.iloc[train_idx].value_counts(sort=False).values\n",
    "            group_val = groups.iloc[val_idx].value_counts(sort=False).values\n",
    "\n",
    "            model = lgb.LGBMRanker(**params)\n",
    "            model.fit(X_train, y_train, group=group_tr,\n",
    "                      eval_set=[(X_val, y_val)], eval_group=[group_val],\n",
    "                      eval_metric='map', eval_at=[7],\n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "            \n",
    "            map_at_7 = model.best_score_['valid_0']['map@7']\n",
    "            oof_map_scores.append(map_at_7)\n",
    "        \n",
    "        mean_score = np.mean(oof_map_scores)\n",
    "        logger.info(f\"Optuna Trial {trial.number} finished with MAP@7: {mean_score}\")\n",
    "        return mean_score\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    logger.info(f\"Best trial MAP@7: {study.best_value}\")\n",
    "    logger.info(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    logger.info(\"Training final model on all data with best parameters.\")\n",
    "    best_params = study.best_params\n",
    "    best_params.update({'objective': 'lambdarank', 'metric': 'map'})\n",
    "    \n",
    "    final_model = lgb.LGBMRanker(**best_params)\n",
    "    group_full = groups.value_counts(sort=False).values\n",
    "    final_model.fit(X, y, group=group_full)\n",
    "    logger.info(\"Final model training complete.\")\n",
    "    \n",
    "    test_preds = final_model.predict(X_test)\n",
    "    \n",
    "    return test_preds, final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T07:51:13.729273Z",
     "iopub.status.busy": "2025-07-16T07:51:13.729018Z",
     "iopub.status.idle": "2025-07-16T07:51:13.749247Z",
     "shell.execute_reply": "2025-07-16T07:51:13.748465Z",
     "shell.execute_reply.started": "2025-07-16T07:51:13.729252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. Model Training with Optuna ---\n",
    "def train_lgbm_with_optuna(train_df, test_df, features, n_trials=10):\n",
    "    \"\"\"Trains a LightGBM Ranker with Optuna for hyperparameter tuning.\"\"\"\n",
    "    logger.info(f\"Starting model training with Optuna for {n_trials} trials.\")\n",
    "    \n",
    "    X = train_df[features].copy()\n",
    "    y = train_df['y'].copy()\n",
    "    X_test = test_df[features].copy()\n",
    "    groups = train_df['id2'].copy()\n",
    "    \n",
    "    # NEW LOGGING: Log data shapes before training\n",
    "    logger.info(f\"Training data shape (X): {X.shape}\")\n",
    "    logger.info(f\"Test data shape (X_test): {X_test.shape}\")\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            median_val = X[col].median()\n",
    "            X[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "    logger.info(\"Filled NaN values using column medians.\") # NEW LOGGING\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'lambdarank', 'metric': 'map', 'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 7, 11),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 50, 150),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-6, 5.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-6, 5.0, log=True),\n",
    "            'random_state': 42, 'verbose': -1, 'bagging_freq': 1\n",
    "        }\n",
    "        \n",
    "        gkf = GroupKFold(n_splits=4)\n",
    "        oof_map_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            group_tr = groups.iloc[train_idx].value_counts(sort=False).values\n",
    "            group_val = groups.iloc[val_idx].value_counts(sort=False).values\n",
    "\n",
    "            model = lgb.LGBMRanker(**params)\n",
    "            model.fit(X_train, y_train, group=group_tr,\n",
    "                      eval_set=[(X_val, y_val)], eval_group=[group_val],\n",
    "                      eval_metric='map', eval_at=[7],\n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "            \n",
    "            map_at_7 = model.best_score_['valid_0']['map@7']\n",
    "            oof_map_scores.append(map_at_7)\n",
    "        \n",
    "        mean_score = np.mean(oof_map_scores)\n",
    "        logger.info(f\"Optuna Trial {trial.number} finished with MAP@7: {mean_score}\") # NEW LOGGING\n",
    "        return mean_score\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    logger.info(f\"Best trial MAP@7: {study.best_value}\")\n",
    "    logger.info(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    logger.info(\"Training final model on all data with best parameters.\")\n",
    "    best_params = study.best_params\n",
    "    best_params.update({'objective': 'lambdarank', 'metric': 'map'})\n",
    "    \n",
    "    final_model = lgb.LGBMRanker(**best_params)\n",
    "    group_full = groups.value_counts(sort=False).values\n",
    "    final_model.fit(X, y, group=group_full)\n",
    "    logger.info(\"Final model training complete.\") # NEW LOGGING\n",
    "    \n",
    "    test_preds = final_model.predict(X_test)\n",
    "    \n",
    "    return test_preds, final_model, study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Submission Generation ---\n",
    "def create_submission(test_df, predictions, filename='submission_robust.csv'):\n",
    "    \"\"\"Creates the submission file in the required format.\"\"\"\n",
    "    logger.info(f\"Creating submission file: {filename}\")\n",
    "    submission = test_df[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "    submission['pred'] = predictions\n",
    "    \n",
    "    # Group-wise normalization for ranking\n",
    "    submission['pred'] = submission.groupby('id2')['pred'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9)\n",
    "    )\n",
    "    submission['id5'] = pd.to_datetime(submission['id5'], errors='coerce').dt.strftime('%m/%d/%y')\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    logger.info(f\"Submission file saved to {filename}. Shape: {submission.shape}\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution function.\"\"\"\n",
    "    logger.info(\"====== Starting Full Robust Pipeline ======\")\n",
    "    \n",
    "    train, test, events, trans, offers = load_and_prepare_data()\n",
    "    \n",
    "    train_fe = create_features(train, offers, trans, events)\n",
    "    test_fe = create_features(test, offers, trans, events)\n",
    "    \n",
    "    train_processed, test_processed = apply_text_and_target_encoding(train_fe, test_fe)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    exclude_cols = ['id1', 'id2', 'id3', 'id4', 'id5', 'id9', 'id10', 'id11', 'id12', 'id13', 'f378', 'start', 'end', 'y']\n",
    "    features = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "    features = [f for f in features if train_processed[f].dtype in ['int64', 'float64', 'int32', 'float32', 'int8', 'bool']]\n",
    "    logger.info(f\"Training with {len(features)} features.\")\n",
    "    \n",
    "    test_predictions, model, _ = train_lgbm_with_optuna(train_processed, test_processed, features)\n",
    "    \n",
    "    _ = create_submission(test_processed, test_predictions)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    logger.info(f\"Top 20 Feature Importances:\\n{importance_df.head(20)}\")\n",
    "    \n",
    "    logger.info(\"====== Pipeline Finished Successfully ======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 17:22:32,151 - INFO - ====== Starting Full Robust Pipeline ======\n",
      "2025-07-20 17:22:32,152 - INFO - Starting data loading and preparation.\n",
      "2025-07-20 17:22:38,655 - INFO - Loaded train_data.parquet with shape: (770164, 372)\n",
      "2025-07-20 17:22:43,122 - INFO - Loaded test_data.parquet with shape: (369301, 371)\n",
      "2025-07-20 17:22:56,211 - INFO - Loaded add_event.parquet with shape: (21457473, 5)\n",
      "2025-07-20 17:22:58,185 - INFO - Loaded add_trans.parquet with shape: (6339465, 9)\n",
      "2025-07-20 17:22:58,374 - INFO - Loaded offer_metadata.parquet with shape: (4164, 12)\n",
      "2025-07-20 17:23:05,100 - INFO - Standardized 'id2' and 'id3' columns to string type.\n",
      "2025-07-20 17:23:05,823 - INFO - Converted timestamp columns to datetime objects.\n",
      "2025-07-20 17:23:05,824 - INFO - Data preparation complete. Final shapes - Train: (770164, 372), Test: (369301, 371)\n",
      "2025-07-20 17:23:05,825 - INFO - Starting feature engineering on dataframe with shape (770164, 372).\n",
      "2025-07-20 17:23:11,479 - INFO - Shape after merging offers: (770164, 383)\n",
      "2025-07-20 17:23:11,510 - INFO - Created basic temporal features: dayofweek, hour.\n",
      "2025-07-20 17:23:11,511 - INFO - Creating id8 (Industry) alignment features.\n",
      "2025-07-20 17:23:21,076 - INFO - Creating behavioral and fatigue features.\n",
      "2025-07-20 17:23:26,993 - INFO - Creating cold-start and global features.\n",
      "2025-07-20 17:23:27,020 - INFO - Creating transaction and event aggregations.\n",
      "2025-07-20 17:23:57,980 - INFO - Feature engineering complete. Final shape: (770164, 400)\n",
      "2025-07-20 17:23:58,004 - INFO - Starting feature engineering on dataframe with shape (369301, 371).\n",
      "2025-07-20 17:24:00,500 - INFO - Shape after merging offers: (369301, 382)\n",
      "2025-07-20 17:24:00,515 - INFO - Created basic temporal features: dayofweek, hour.\n",
      "2025-07-20 17:24:00,516 - INFO - Creating id8 (Industry) alignment features.\n",
      "2025-07-20 17:24:06,483 - INFO - Creating behavioral and fatigue features.\n",
      "2025-07-20 17:24:09,831 - INFO - Creating cold-start and global features.\n",
      "2025-07-20 17:24:09,845 - INFO - Creating transaction and event aggregations.\n",
      "2025-07-20 17:24:24,918 - INFO - Feature engineering complete. Final shape: (369301, 399)\n",
      "2025-07-20 17:24:24,941 - INFO - Applying TF-IDF and Target Encoding.\n",
      "2025-07-20 17:24:32,561 - INFO - Applied TF-IDF. Train shape: (770164, 430), Test shape: (369301, 429)\n",
      "2025-07-20 17:24:32,562 - INFO - Starting GroupKFold Target Encoding for 'id3'.\n",
      "2025-07-20 17:24:32,931 - INFO - Processing fold 1/5 for target encoding.\n",
      "2025-07-20 17:24:35,163 - INFO - Processing fold 2/5 for target encoding.\n",
      "2025-07-20 17:24:37,396 - INFO - Processing fold 3/5 for target encoding.\n",
      "2025-07-20 17:24:39,628 - INFO - Processing fold 4/5 for target encoding.\n",
      "2025-07-20 17:24:41,853 - INFO - Processing fold 5/5 for target encoding.\n",
      "2025-07-20 17:24:44,134 - INFO - Target encoding complete.\n",
      "2025-07-20 17:24:44,191 - INFO - Training with 51 features.\n",
      "2025-07-20 17:24:44,192 - INFO - Starting model training with Optuna for 10 trials.\n",
      "2025-07-20 17:24:44,254 - INFO - Training data shape (X): (770164, 51)\n",
      "2025-07-20 17:24:44,255 - INFO - Test data shape (X_test): (369301, 51)\n",
      "2025-07-20 17:24:44,355 - INFO - Filled NaN values using column medians.\n",
      "[I 2025-07-20 17:24:44,356] A new study created in memory with name: no-name-7f49a5ff-ea32-4873-a452-462c9afb7c54\n",
      "2025-07-20 17:25:01,236 - INFO - Optuna Trial 0 finished with MAP@7: 0.9523295711284229\n",
      "[I 2025-07-20 17:25:01,238] Trial 0 finished with value: 0.9523295711284229 and parameters: {'n_estimators': 774, 'learning_rate': 0.02547372140115714, 'num_leaves': 80, 'max_depth': 11, 'min_child_samples': 127, 'feature_fraction': 0.8877107952124741, 'bagging_fraction': 0.9775575414847388, 'lambda_l1': 4.530082784368556e-06, 'lambda_l2': 0.5346302138555798}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:25:16,686 - INFO - Optuna Trial 1 finished with MAP@7: 0.9521526168694519\n",
      "[I 2025-07-20 17:25:16,688] Trial 1 finished with value: 0.9521526168694519 and parameters: {'n_estimators': 1496, 'learning_rate': 0.024848463854237335, 'num_leaves': 38, 'max_depth': 7, 'min_child_samples': 142, 'feature_fraction': 0.76041279266649, 'bagging_fraction': 0.8734667933421948, 'lambda_l1': 1.480583070190272e-05, 'lambda_l2': 0.02734979692158764}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:25:30,475 - INFO - Optuna Trial 2 finished with MAP@7: 0.9522136960077299\n",
      "[I 2025-07-20 17:25:30,477] Trial 2 finished with value: 0.9522136960077299 and parameters: {'n_estimators': 1220, 'learning_rate': 0.04213533335627866, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 146, 'feature_fraction': 0.9975984194006327, 'bagging_fraction': 0.7388961517418992, 'lambda_l1': 0.00019381212187310757, 'lambda_l2': 1.0084124596865447e-06}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:25:43,266 - INFO - Optuna Trial 3 finished with MAP@7: 0.9518746982497249\n",
      "[I 2025-07-20 17:25:43,267] Trial 3 finished with value: 0.9518746982497249 and parameters: {'n_estimators': 1560, 'learning_rate': 0.014357427233749989, 'num_leaves': 57, 'max_depth': 10, 'min_child_samples': 88, 'feature_fraction': 0.739085933797253, 'bagging_fraction': 0.8872452803177152, 'lambda_l1': 4.449219966301144e-05, 'lambda_l2': 0.0012250394122427066}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:25:53,097 - INFO - Optuna Trial 4 finished with MAP@7: 0.9517375665919473\n",
      "[I 2025-07-20 17:25:53,099] Trial 4 finished with value: 0.9517375665919473 and parameters: {'n_estimators': 1877, 'learning_rate': 0.011044100933725164, 'num_leaves': 86, 'max_depth': 9, 'min_child_samples': 134, 'feature_fraction': 0.9445273649595983, 'bagging_fraction': 0.7901677451679024, 'lambda_l1': 1.1073430573760958, 'lambda_l2': 0.0021131005102160713}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:26:04,370 - INFO - Optuna Trial 5 finished with MAP@7: 0.9522836280039632\n",
      "[I 2025-07-20 17:26:04,371] Trial 5 finished with value: 0.9522836280039632 and parameters: {'n_estimators': 1033, 'learning_rate': 0.043497885064139874, 'num_leaves': 92, 'max_depth': 10, 'min_child_samples': 103, 'feature_fraction': 0.9491964033103548, 'bagging_fraction': 0.8597344095693734, 'lambda_l1': 0.046437122538619084, 'lambda_l2': 0.9389047349623493}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:26:14,918 - INFO - Optuna Trial 6 finished with MAP@7: 0.9522732574792977\n",
      "[I 2025-07-20 17:26:14,920] Trial 6 finished with value: 0.9522732574792977 and parameters: {'n_estimators': 1698, 'learning_rate': 0.03514184326409038, 'num_leaves': 76, 'max_depth': 10, 'min_child_samples': 142, 'feature_fraction': 0.9065163090324397, 'bagging_fraction': 0.9112636068454605, 'lambda_l1': 0.09031712342181972, 'lambda_l2': 0.12375302997072281}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:26:34,773 - INFO - Optuna Trial 7 finished with MAP@7: 0.9520608596881901\n",
      "[I 2025-07-20 17:26:34,775] Trial 7 finished with value: 0.9520608596881901 and parameters: {'n_estimators': 576, 'learning_rate': 0.015370374321247354, 'num_leaves': 48, 'max_depth': 10, 'min_child_samples': 53, 'feature_fraction': 0.9854784557401743, 'bagging_fraction': 0.7014856011535843, 'lambda_l1': 8.021554670901459e-06, 'lambda_l2': 1.7665220910903463}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:26:51,053 - INFO - Optuna Trial 8 finished with MAP@7: 0.9519849440744272\n",
      "[I 2025-07-20 17:26:51,054] Trial 8 finished with value: 0.9519849440744272 and parameters: {'n_estimators': 834, 'learning_rate': 0.01604363355822581, 'num_leaves': 61, 'max_depth': 8, 'min_child_samples': 144, 'feature_fraction': 0.7311145797379796, 'bagging_fraction': 0.8622223833114957, 'lambda_l1': 1.3574486528318279e-06, 'lambda_l2': 0.013760043584338508}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:27:10,815 - INFO - Optuna Trial 9 finished with MAP@7: 0.9518880128432807\n",
      "[I 2025-07-20 17:27:10,817] Trial 9 finished with value: 0.9518880128432807 and parameters: {'n_estimators': 1729, 'learning_rate': 0.013478002542752953, 'num_leaves': 40, 'max_depth': 10, 'min_child_samples': 63, 'feature_fraction': 0.7268235648952582, 'bagging_fraction': 0.9404385000909101, 'lambda_l1': 0.21467906775251017, 'lambda_l2': 3.145615760910426e-05}. Best is trial 0 with value: 0.9523295711284229.\n",
      "2025-07-20 17:27:10,817 - INFO - Best trial MAP@7: 0.9523295711284229\n",
      "2025-07-20 17:27:10,818 - INFO - Best params: {'n_estimators': 774, 'learning_rate': 0.02547372140115714, 'num_leaves': 80, 'max_depth': 11, 'min_child_samples': 127, 'feature_fraction': 0.8877107952124741, 'bagging_fraction': 0.9775575414847388, 'lambda_l1': 4.530082784368556e-06, 'lambda_l2': 0.5346302138555798}\n",
      "2025-07-20 17:27:10,818 - INFO - Training final model on all data with best parameters.\n",
      "2025-07-20 17:27:23,118 - INFO - Final model training complete.\n",
      "2025-07-20 17:27:25,572 - INFO - Creating submission file: submission_robust.csv\n",
      "2025-07-20 17:27:32,439 - INFO - Submission file saved to submission_robust.csv. Shape: (369301, 5)\n",
      "2025-07-20 17:27:32,443 - INFO - Top 20 Feature Importances:\n",
      "                        feature  importance\n",
      "9   session_impression_gap_secs       11426\n",
      "19     customer_offer_frequency       11126\n",
      "50                       id3_te        8729\n",
      "4                          hour        6136\n",
      "18             offer_popularity        6066\n",
      "10               offer_age_days        5911\n",
      "11          offer_duration_days        3754\n",
      "3                     dayofweek        1620\n",
      "29                      tfidf_9        1008\n",
      "48                     tfidf_28         961\n",
      "1                          f376         955\n",
      "8         campaign_fatigue_user         739\n",
      "47                     tfidf_27         507\n",
      "28                      tfidf_8         386\n",
      "20                      tfidf_0         310\n",
      "49                     tfidf_29         252\n",
      "44                     tfidf_24         216\n",
      "7            offer_fatigue_user         170\n",
      "0                          f375         149\n",
      "32                     tfidf_12          74\n",
      "2025-07-20 17:27:32,443 - INFO - ====== Pipeline Finished Successfully ======\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7828305,
     "sourceId": 12412608,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7828911,
     "sourceId": 12413499,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
